<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="ARNOLD: A Benchmark for Language-Grounded Task Learning With Continuous States in Realistic 3D Scenes">
  <meta name="keywords" content="Benchmark, Language Grounding, Manipulation, Realistic, Continuous States">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ARNOLD</title>

<!-- Add icon library -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css">
<style>
.btn {
  background-color: Gray;
  border: none;
  color: white;
  padding: 12px 16px;
  font-size: 16px;
  cursor: pointer;
}

.is-rounded {border-radius: 12px;}

/* Darker background on mouse-over */
.btn:hover {
  background-color: Silver;
}

a {
	color: inherit;
    text-decoration: none;
}

.centered {
  text-align: center;
}
</style>

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/bulma/0.7.5/css/bulma.min.css">
<script src="https://kit.fontawesome.com/b25d0fd750.js" crossorigin="anonymous"></script>
<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>

</head>

<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">ARNOLD: A Benchmark for Language-Grounded Task Learning With Continuous States in Realistic 3D Scenes</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a target="_blank" href="https://nikepupu.github.io/">Ran Gong</a><sup>1✶</sup>,</span>
            <span class="author-block">
              <a target="_blank" href="https://huangjy-pku.github.io/">Jiangyong Huang</a><sup>2,5✶</sup>,</span>
            <span class="author-block">
              <a target="_blank" href="https://www.zyz.lol/app/aboutme/aboutme.html">Yizhou Zhao</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a target="_blank" href="https://geng-haoran.github.io/">Haoran Geng</a><sup>2,5</sup>,
            </span>
            <span class="author-block">
              <a target="_blank" href="https://xfgao.github.io/">Xiaofeng Gao</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a target="_blank" href="https://qywu.github.io/">Qingyang Wu</a><sup>4</sup>,
            </span>
            </br>
            <span class="author-block">
              <a target="_blank" href="https://wensi-ai.github.io/">Wensi Ai</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a target="_blank" href="https://www.linkedin.com/in/josephziheng/">Ziheng Zhou</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a target="_blank" href="http://web.cs.ucla.edu/~dt/">Demetri Terzopoulos</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a target="_blank" href="http://www.stat.ucla.edu/~sczhu/">Song-Chun Zhu</a><sup>2,3,5</sup>,
            </span>
            <span class="author-block">
              <a target="_blank" href="https://buzz-beater.github.io/">Baoxiong Jia</a><sup>5</sup>,
            </span>
            <span class="author-block">
              <a target="_blank" href="https://siyuanhuang.com/">Siyuan Huang</a><sup>5</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>University of California, Los Angeles,</span>
            <span class="author-block"><sup>2</sup>Peking University,</span>
            <span class="author-block"><sup>3</sup>Tsinghua University,</span>
            </br>
            <span class="author-block"><sup>4</sup>Columbia University,</span>
            <span class="author-block"><sup>5</sup>National Key Laboratory of General Artificial Intelligence, BIGAI</span>
          </div>

          <p style="font-size: 0.9em; padding: 0.5em 0 0 0;">✶ indicates equal contribution</p>

          <div class="column has-text-centered">
            <div class="publication-links">
            <!-- Arxiv Link. -->
            <span class="link-block">
              <a target="_blank" href="placeholder"
                 class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                    <i class="fas fa-file"></i>
                </span>
                <span>arXiv</span>
              </a>
            </span>

            <!-- Code Link. -->
            <span class="link-block">
              <a target="_blank" href="https://github.com/arnold-benchmark/arnold"
                 class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                    <i class="fab fa-github"></i>
                </span>
                <span>Code</span>
                </a>
            </span>

            <!-- Video Link. -->
            <span class="link-block">
              <a target="_blank" href="https://youtu.be/w-Cp1PRDWzI"
                 class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                    <i class="fab fa-youtube"></i>
                </span>
                <span>Video</span>
              </a>
            </span>

            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-fullhd">
    <div class="hero-body">
      <div class="container">
        <div class="has-text-centered" style="padding: 0 1em;">
          <img src="assets/teaser.png"/>
          <p style="text-align: justify; font-size: 0.8em;">We present <tt>ARNOLD</tt>, a benchmark that evaluates <b>language-grounded</b> task learning with <b>continuous states</b> in <b>realistic 3D scenes</b>. <tt>ARNOLD</tt> provides 8 tasks with their demonstrations for learning and a testbed for the generalization abilities of agents over (1) <span style="color: #DB3A34;">novel goal states</span>, (2) <span style="color: #963484;">novel objects</span>, and (3) <span style="color: #1AA260;">novel scenes</span>.</p>
        </div>
      </br>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Understanding the continuous states of objects is essential for task learning and planning in the real world. However, most existing task learning benchmarks assume discrete (<i>e.g.</i>, binary) object goal states, which poses challenges for the learning of complex tasks and transferring learned policy from simulated environments to the real world. Furthermore, state discretization limits a robot's ability to follow human instructions based on the grounding of actions and states. To tackle these challenges, we present <tt>ARNOLD</tt>, a benchmark that evaluates language-grounded task learning with continuous states in realistic 3D scenes. <tt>ARNOLD</tt> is comprised of 8 language-conditioned tasks that involve understanding object states and learning policies for continuous goals. To promote language-instructed learning, we provide expert demonstrations with template-generated language descriptions. We assess task performance by utilizing the latest language-conditioned policy learning models. Our results indicate that current models for language-conditioned manipulations continue to experience significant challenges in novel goal-state generalizations, scene generalizations, and object generalizations. These findings highlight the need to develop new algorithms that address this gap and underscore the potential for further research in this area.
          </p>
        </div>
      </div>
    </div>
    <br>
    <br>
    <!--/ Abstract. -->

  </div>

  <!-- Paper video. -->
  <div class="columns is-centered has-text-centered">
    <div class="column is-two-thirds">
      <h2 class="title is-3">Video</h2>
      <div class="publication-video">
        <iframe src="https://www.youtube.com/embed/w-Cp1PRDWzI" frameborder="0" allow="autoplay; encrypted-media; picture-in-picture" allowfullscreen></iframe>
      </div>
    </div>
  </div>

</section>

<section class="section">
  <div class="container is-max-widescreen">

    <div class="rows">


    <!-- Animation. -->
    <div class="rows is-centered ">
      <div class="row is-full-width">
        <!-- Interpolating. -->
        <h3 class="title is-4">Summary</h3>
        <div class="content has-text-justified">
        <!-- <br> -->
        </div>
        <p>
          We highlight the following major points: (1) <tt>ARNOLD</tt> is built on <b>NVIDIA Isaac Sim</b>, equipped with <b>photo-realistic</b> and <b>physically-accurate</b> simulation, covering <b>40 distinctive objects</b> and <b>20 scenes</b>. (2) <tt>ARNOLD</tt> is comprised of <b>8 language-conditioned tasks</b> that involve understanding object states and learning policies for continuous goals. For each task, there are <b>7 data splits</b> including <i>i.i.d.</i> evaluation and <b>unseen generalization</b>. (3) <tt>ARNOLD</tt> provides <b>10k expert demonstrations</b> with diverse template-generated language instructions, based on thousands of human annotations. (4) We assess the task performances of the latest language-conditioned policy learning models. The results indicate that current models for language-conditioned manipulation <b>still struggle in understanding continuous states and producing precise motion control</b>. We hope these findings can foster future research to address the unsolved challenges in <b>instruction grounding</b> and <b>precise continuous motion control</b>.
        </p>
        </br>
        </br>

        <h3 class="title is-4">Comparison</h3>
        <div class="has-text-centered" style="padding: 0 1em;">
          <p style="text-align: justify; font-size: 0.8em;"><tt>ARNOLD</tt> features continuous robot control over continuous object states with a large
            number of demonstrations in photo-realistic scenes. Each task in <tt>ARNOLD</tt> is specified by a natural language instruction. <tt>ARNOLD</tt> also
            leverages advanced physics simulations powered by PhysX 5.0 to simulate articulated bodies and fluids. <b>Language</b>: Task goals are specified
            by natural language instruction. <b>Multi-Camera</b>: Robot is equipped with multiple cameras. <sup>1</sup>: A number of tasks in Maniskill use ground
            truth semantic segmentation as input. <b>Fluid</b>: Advanced fluid simulation. <b>Physics</b>: Realistic physics simulation with realistic grasping.
            <sup>2</sup>: RLbench-based benchmarks use simplified grasping. <b>Continuous</b>: Object state and goal state are continuous. <b>Scene</b>: Tasks are performed
            with a realistic scene background. <b>Robot</b>: Perform actions with real robots for all tasks. <b>R</b>: Rasterization. <b>RT</b>: RayTracing. <b>Flexible
            Material</b>: Easy to change materials and textures. <b>Generalization</b>: Systematic generalization test at different levels. Note that although
            Maniskill shows 3D scenes in their illustration figure, they do not include 3D scenes in their benchmark.
          <br/>
          <img src="assets/comparison.png"/>
        </div>
        <br/>
        <br/>

        <h3 class="title is-4">Results</h3>

        <div class="columns">
          <div class="column has-text-centered">
            <h3 class="title is-5">One Multi-Task Transformer</h3>

            Trained with
            <div class="select is-small">
              <select id="single-menu-demos" onchange="updateSingleVideo()">
              <option value="10">10</option>
              <option value="100" selected="selected">100</option>
              </select>
            </div>
            demos per task, evaluated on 
            <div class="select is-small">     
              <select id="single-menu-tasks" onchange="updateSingleVideo()">
              <option value="open_drawer" selected="selected">open drawer</option>
              <option value="slide_block">slide block</option>
              <option value="sweep_to_dustpan">sweep to dustpan</option>
              <option value="meat_off_grill">meat off grill</option>
              <option value="turn_tap">turn tap</option>
              <option value="put_in_drawer">put in drawer</option>
              <option value="close_jar">close jar</option>
              <option value="drag_stick">drag stick</option>
              <option value="stack_blocks">stack blocks</option>
              <option value="screw_bulb">screw bulb</option>
              <option value="put_in_safe">put in safe</option>
              <option value="place_wine">place wine</option>
              <option value="put_in_cupboard">put in cupboard</option>
              <option value="sort_shape">sort shape</option>
              <option value="push_buttons">push buttons</option>
              <option value="insert_peg">insert peg</option>
              <option value="stack_cups">stack cups</option>
              <option value="place_cups">place cups</option>
              </select>
            </div>
            episode
            <div class="select is-small">
              <select id="single-menu-instances" onchange="updateSingleVideo()">
              <option value="s1">01</option>
              <option value="s2" selected="selected">02</option>
              <option value="s3">03</option>
              <option value="s4">04</option>
              <option value="s5">05</option>
              </select>
            </div>
            <br/>
            <br/>

            <video id="multi-task-result-video"
                   muted
                   autoplay
                   loop
                   width="100%">
              <source src="media/results/sim_rollouts/n10-open_drawer-s2.mp4"
                      type="video/mp4">
            </video>
          </div>
        </div>
        </br>
        </br>

        <h3 class="title is-4">Action Predictions</h3>

        <div class="columns">
          <div class="column has-text-centered">
            <h3 class="title is-5">Q-Prediction Examples</h3>

            Visualize predictions for   
            <div class="select is-small is-rounded">     
              <select id="single-menu-qpred" onchange="updateQpredVideo()">
              <option value="tomato" selected="selected">"put the tomatoes in the top bin"</option>
              <option value="stick">"hit the green ball with the stick"</option>
              <option value="handsan">"press the hand san"</option>
              <option value="tape">"put the tape in the top drawer"</option>
              </select>
            </div>
          </div>

      </div>
    </div>
  </div>
</section>

<video id="q-pred-video"
       muted
       autoplay
       loop
       width="100%">
  <source src="media/results/qpred/tomato.mp4"
          type="video/mp4">
</video>

<br>
<br>
<section class="section">
  <div class="container is-max-widescreen">

    <div class="rows">
      <h2 class="title is-3">Emergent Properties</h2>

      <h3 class="title is-4">Tracking Objects</h3>
      A selected example of tracking an unseen hand sanitizer instance with an agent that was trained on a single object with 5 "press the handsan" demos. Since PerAct focuses on actions, it doesn't need a complete representation of the bottle, and only has to predict <b><i>where</i> to press</b> the sanitizer.

      <video id="tracking-objects"
             muted
             autoplay
             loop
             width="99%">
        <source src="media/results/animations/handsan_tracking_v2.mp4" 
                type="video/mp4">
      </video>

    </div>

  </div>

</section>


<section class="section" id="BibTeX">
  <div class="container is-max-widescreen content">
    <h2 class="title">BibTeX</h2>
    <pre><code>Placeholder</code></pre>
  </div>
</section>


</body>
</html>
