<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="ARNOLD: A Benchmark for Language-Grounded Task Learning With Continuous States in Realistic 3D Scenes">
  <meta name="keywords" content="Benchmark, Language Grounding, Manipulation, Realistic, Continuous States">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>ARNOLD</title>

<!-- Add icon library -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.3/css/all.min.css">
<style>
.btn {
  background-color: Gray;
  border: none;
  color: white;
  padding: 12px 16px;
  font-size: 16px;
  cursor: pointer;
}

.is-rounded {border-radius: 12px;}

/* Darker background on mouse-over */
.btn:hover {
  background-color: Silver;
}

a {
	color: inherit;
    text-decoration: none;
}

.centered {
  text-align: center;
}

.publication-video {
  position: relative;
  width: 100%;
  height: 0;
  padding-bottom: 56.25%;

  overflow: hidden;
  border-radius: 10px !important;
}

.publication-video iframe {
  position: absolute;
  top: 0;
  left: 0;
  width: 100%;
  height: 100%;
}

</style>

<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/bulma/0.7.5/css/bulma.min.css">
<script src="https://kit.fontawesome.com/b25d0fd750.js" crossorigin="anonymous"></script>
<script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>

<script>
function UpdateTaskIllustration() {
  var task = document.getElementById("illu-task").value;
  console.log("illu", task)
  var img = document.getElementById("task-illustration");
  img.src = "assets/" + "illu-" + task + ".png"
}

function UpdateTaskDemonstration() {
  var task = document.getElementById("demo-task").value;
  console.log("demo", task)
  var video = document.getElementById("task-demonstration");
  video.src = "assets/" + "demo-" + task + ".mp4"
  video.playbackRate = 1.75;
  video.play();
}
</script>

</head>

<body onload="UpdateTaskIllustration(); UpdateTaskDemonstration();">

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">ARNOLD: A Benchmark for Language-Grounded Task Learning With Continuous States in Realistic 3D Scenes</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a target="_blank" href="https://nikepupu.github.io/">Ran Gong</a><sup>1✶</sup>,</span>
            <span class="author-block">
              <a target="_blank" href="https://huangjy-pku.github.io/">Jiangyong Huang</a><sup>2,5✶</sup>,</span>
            <span class="author-block">
              <a target="_blank" href="https://www.zyz.lol/app/aboutme/aboutme.html">Yizhou Zhao</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a target="_blank" href="https://geng-haoran.github.io/">Haoran Geng</a><sup>2,5</sup>,
            </span>
            <span class="author-block">
              <a target="_blank" href="https://xfgao.github.io/">Xiaofeng Gao</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a target="_blank" href="https://qywu.github.io/">Qingyang Wu</a><sup>4</sup>,
            </span>
            <br/>
            <span class="author-block">
              <a target="_blank" href="https://wensi-ai.github.io/">Wensi Ai</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a target="_blank" href="https://www.linkedin.com/in/josephziheng/">Ziheng Zhou</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a target="_blank" href="http://web.cs.ucla.edu/~dt/">Demetri Terzopoulos</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a target="_blank" href="http://www.stat.ucla.edu/~sczhu/">Song-Chun Zhu</a><sup>2,3,5</sup>,
            </span>
            <span class="author-block">
              <a target="_blank" href="https://buzz-beater.github.io/">Baoxiong Jia</a><sup>5</sup>,
            </span>
            <span class="author-block">
              <a target="_blank" href="https://siyuanhuang.com/">Siyuan Huang</a><sup>5</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>University of California, Los Angeles,</span>
            <span class="author-block"><sup>2</sup>Peking University,</span>
            <span class="author-block"><sup>3</sup>Tsinghua University,</span>
            <br/>
            <span class="author-block"><sup>4</sup>Columbia University,</span>
            <span class="author-block"><sup>5</sup>National Key Laboratory of General Artificial Intelligence, BIGAI</span>
          </div>

          <p style="font-size: 0.9em; padding: 0.5em 0 0 0;">✶ indicates equal contribution</p>

          <div class="column has-text-centered">
            <div class="publication-links">
            <!-- Arxiv Link. -->
            <span class="link-block">
              <a target="_blank" href="http://arxiv.org/abs/2304.04321"
                 class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                    <i class="fas fa-file"></i>
                </span>
                <span>arXiv</span>
              </a>
            </span>

            <!-- Code Link. -->
            <span class="link-block">
              <a target="_blank" href="https://github.com/arnold-benchmark/arnold"
                 class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                    <i class="fab fa-github"></i>
                </span>
                <span>Code</span>
              </a>
            </span>

            <!-- Video Link. -->
            <span class="link-block">
              <a target="_blank" href="https://youtu.be/w-Cp1PRDWzI"
                 class="external-link button is-normal is-rounded is-dark">
                <span class="icon">
                    <i class="fab fa-youtube"></i>
                </span>
                <span>Video</span>
              </a>
            </span>

            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser" style="margin-top: -5rem;">
  <div class="container is-fullhd">
    <div class="hero-body">
      <div class="container">
        <div class="has-text-centered" style="padding: 0 1em;">
          <img src="assets/teaser.png"/>
          <p style="text-align: justify; font-size: 0.8em;">We present <tt>ARNOLD</tt>, a benchmark that evaluates <b>language-grounded</b> task learning with <span style="color: #2176FF;"><b>continuous states</b></span> in <b>realistic 3D scenes</b>. <tt>ARNOLD</tt> provides 8 tasks with their demonstrations for learning and a testbed for the generalization abilities of agents over (1) <span style="color: #DB3A34;"><b>novel goal states</b></span>, (2) <span style="color: #963484;"><b>novel objects</b></span>, and (3) <span style="color: #1AA260;"><b>novel scenes</b></span>.</p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section" style="margin-top: -4rem;">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Understanding the continuous states of objects is essential for task learning and planning in the real world. However, most existing task learning benchmarks assume discrete (<i>e.g.</i>, binary) object goal states, which poses challenges for the learning of complex tasks and transferring learned policy from simulated environments to the real world. Furthermore, state discretization limits a robot's ability to follow human instructions based on the grounding of actions and states. To tackle these challenges, we present <tt>ARNOLD</tt>, a benchmark that evaluates language-grounded task learning with continuous states in realistic 3D scenes. <tt>ARNOLD</tt> is comprised of 8 language-conditioned tasks that involve understanding object states and learning policies for continuous goals. To promote language-instructed learning, we provide expert demonstrations with template-generated language descriptions. We assess task performance by utilizing the latest language-conditioned policy learning models. Our results indicate that current models for language-conditioned manipulations continue to experience significant challenges in novel goal-state generalizations, scene generalizations, and object generalizations. These findings highlight the need to develop new algorithms that address this gap and underscore the potential for further research in this area.
          </p>
        </div>
      </div>
    </div>
    <br>
    <br>
    <!--/ Abstract. -->

  </div>

  <!-- Paper video. -->
  <div class="columns is-centered has-text-centered" style="margin-top: -3rem;">
    <div class="column is-two-thirds">
      <h2 class="title is-3">Video</h2>
      <div class="publication-video">
        <iframe src="https://www.youtube.com/embed/w-Cp1PRDWzI" frameborder="0" allow="autoplay; encrypted-media; picture-in-picture" allowfullscreen></iframe>
      </div>
    </div>
  </div>

</section>

<section class="section">
  <div class="container is-max-widescreen">

    <div class="rows">

    <!-- Animation. -->
    <div class="rows is-centered ">
      <div class="row is-full-width">
        <!-- Interpolating. -->
        <h3 class="title is-4">Summary</h3>
        <div class="content has-text-justified">
        <!-- <br> -->
        </div>
        <p>
          We highlight the following major points: (1) <tt>ARNOLD</tt> is built on <b>NVIDIA Isaac Sim</b>, equipped with <b>photo-realistic</b> and <b>physically-accurate</b> simulation, covering <b>40 distinctive objects</b> and <b>20 scenes</b>. (2) <tt>ARNOLD</tt> is comprised of <b>8 language-conditioned tasks</b> that involve understanding object states and learning policies for continuous goals. For each task, there are <b>7 data splits</b> including <i>i.i.d.</i> evaluation and <b>unseen generalization</b>. (3) <tt>ARNOLD</tt> provides <b>10k expert demonstrations</b> with diverse template-generated language instructions, based on thousands of human annotations. (4) We assess the task performances of the latest language-conditioned policy learning models. The results indicate that current models for language-conditioned manipulation <b>still struggle in understanding continuous states and producing precise motion control</b>. We hope these findings can foster future research to address the unsolved challenges in <b>instruction grounding</b> and <b>precise continuous motion control</b>.
        </p>
        <br/>
        <br/>

        <h3 class="title is-4">Comparison</h3>
        <div class="has-text-centered" style="padding: 0em; width:80%; margin: 0 auto;">
          <p style="text-align: justify; font-size: 0.8em;"><tt>ARNOLD</tt> features continuous robot control over continuous object states with a large
            number of demonstrations in photo-realistic scenes. Each task in <tt>ARNOLD</tt> is specified by a natural language instruction. <tt>ARNOLD</tt> also
            leverages advanced physics simulations powered by PhysX 5.0 to simulate articulated bodies and fluids. <b>Language</b>: Task goals are specified
            by natural language instruction. <b>Multi-Camera</b>: Robot is equipped with multiple cameras. <sup>1</sup>: A number of tasks in Maniskill use ground
            truth semantic segmentation as input. <b>Fluid</b>: Advanced fluid simulation. <b>Physics</b>: Realistic physics simulation with realistic grasping.
            <sup>2</sup>: RLbench-based benchmarks use simplified grasping. <b>Continuous</b>: Object state and goal state are continuous. <b>Scene</b>: Tasks are performed
            with a realistic scene background. <b>Robot</b>: Perform actions with real robots for all tasks. <b>R</b>: Rasterization. <b>RT</b>: RayTracing. <b>Flexible
            Material</b>: Easy to change materials and textures. <b>Generalization</b>: Systematic generalization test at different levels.
          </p>
          <br/>
          <img src="assets/comparison.png" style="margin-top: -1rem;"/>
        </div>
        <br/>
        <br/>

        <h3 class="title is-4">Simulation Environment</h3>
        <div style="display:flex;">
          <div style="width:60%;">
            <p><tt>ARNOLD</tt> is built on <a target="_blank" href="https://developer.nvidia.com/isaac-sim">Isaac Sim</a>, featuring photo-realistic and physically-accurate simulation.
              The photo-realistic rendering is powered by GPU-enabled ray tracing, and the physics simulation is based on PhysX 5.0.
              In <tt>ARNOLD</tt>, we assign physics parameters (<i>e.g.</i>, friction, surface tension) to objects, including rigid-body objects and fluids.
              <tt>ARNOLD</tt> covers 40 distinct objects and 20 diverse scenes. The scenes are curated from <a target="_blank" href="https://tianchi.aliyun.com/specials/promotion/alibaba-3d-scene-dataset">3D-FRONT</a>.
              The objects come from <a target="_blank" href="https://developer.nvidia.com/isaac-sim">Isaac Sim</a>, <a target="_blank" href="https://ai2thor.allenai.org/">AI2-THOR</a>, <a target="_blank" href="https://sapien.ucsd.edu/">SAPIEN</a>.
              To enhance visual realism, we modified object meshes, <i>e.g.</i>, by modifying materials and adding top covers to drawers and cabinets.
              For more stable physics-based grasping, we performed convex decomposition to create precise collision proxies for each object.
              We use a 7-DoF Franka Emika Panda manipulator with a parallel gripper in <tt>ARNOLD</tt> for task execution.
              There are 5 cameras around the robot to provide visual inputs.
              We show an example of camera rendering, and visualize objects/scenes as follows.
            </p>
          </div>

          <div style="width:5%; display:flex; justify-content:center; align-items:center;">
          </div>

          <div style="width:30%; display:flex; justify-content:center; align-items:center;">
            <img src="assets/multi_view.png" alt="Multi-view cameras">
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div class="columns">
        <div class="column is-half">
          <video poster="" id="scene1" autoplay muted loop height="100%">
            <source src="assets/scene_1.mp4" type="video/mp4">
          </video>
        </div>
        <div class="column is-half">
          <video poster="" id="scene2" autoplay muted loop height="100%">
            <source src="assets/scene_2.mp4" type="video/mp4">
          </video>
        </div>
      </div>

      <div class="columns">
        <div class="column is-half">
          <video poster="" id="scene3" autoplay muted loop height="100%">
            <source src="assets/scene_3.mp4" type="video/mp4">
          </video>
        </div>
        <div class="column is-half">
          <video poster="" id="scene4" autoplay muted loop height="100%">
            <source src="assets/scene_4.mp4" type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero is-light is-small">
  <div class="hero-body">
    <div class="container">
      <div class="columns">
        <div class="column is-half">
          <video poster="" id="object1" autoplay muted loop height="100%">
            <source src="assets/bottle.mp4" type="video/mp4">
          </video>
        </div>
        <div class="column is-half">
          <video poster="" id="object2" autoplay muted loop height="100%">
            <source src="assets/drawer.mp4" type="video/mp4">
          </video>
        </div>
      </div>
      <div class="columns">
        <div class="column is-half">
          <video poster="" id="object3" autoplay muted loop height="100%">
            <source src="assets/cabinet.mp4" type="video/mp4">
          </video>
        </div>
        <div class="column is-half">
          <video poster="" id="object4" autoplay muted loop height="100%">
            <source src="assets/cup.mp4" type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-widescreen">
    <div class="rows">
    <div class="rows is-centered ">
      <div class="row is-full-width">
        <h3 class="title is-4">Task</h3>
        <div style="display:flex;">
          <div style="width:50%;">
            <p><tt>ARNOLD</tt> contains 8 tasks with various goal state varitions.
              To succeed, the robot has to manipulate the configurations to maintain the object state within a continuous range around the goal state for a while.
              Accomplishing these tasks requires capabilities in language grounding, friction-based
              grasping, continuous state understanding, and precise robot motion control.
            </p>
          </div>

          <div style="width:10%; display:flex; justify-content:center; align-items:center;">
          </div>

          <div style="width:32%; display:flex; justify-content:center; align-items:center;">
            <img src="assets/tasks.png" alt="Task overview">
          </div>
        </div>
        
        <div class="column has-text-centered" style="margin-top: 1rem;">
        Illustration of task 
          <div class="select is-small">     
            <select id="illu-task" onchange="UpdateTaskIllustration()">
            <option value="pickup_object" selected="selected">Pickup Object</option>
            <option value="reorient_object">Reorient Object</option>
            <option value="open_drawer">Open Drawer</option>
            <option value="close_drawer">Close Drawer</option>
            <option value="open_cabinet">Open Cabinet</option>
            <option value="close_cabinet">Close Cabinet</option>
            <option value="pour_water">Pour Water</option>
            <option value="transfer_water">Transfer Water</option>
            </select>
          </div>
          <br/>
          <br/>

          <img id="task-illustration" src="assets/illu-pickup_object.png" width="100%" alt="Task illustration">
        </div>

        <div class="column has-text-centered" style="margin-top: 1rem;">
          Demonstration of task 
            <div class="select is-small">     
              <select id="demo-task" onchange="UpdateTaskDemonstration()">
              <option value="pickup_object" selected="selected">Pickup Object</option>
              <option value="close_drawer">Close Drawer</option>
              <option value="open_cabinet">Open Cabinet</option>
              <option value="transfer_water">Transfer Water</option>
              </select>
            </div>
            <video id="task-demonstration"
                    muted
                    autoplay
                    loop
                    width="80%">
              <source src="assets/demo-pickup_object.mp4"
                      type="video/mp4">
            </video>
        </div>

      </div>
    </div>
  </div>
</section>


<section class="section" style="margin-top: -2rem;">
  <div class="container is-max-widescreen">
    <div class="rows">
    <div class="rows is-centered ">
      <div class="row is-full-width">
        <h3 class="title is-4">Benchmark</h3>
        <p>
          To generate demonstrations for the tasks, we design several motion planners, which execute actions on sub-task stages.
          While motion planning is challenging in particular tasks (<i>e.g.</i>, pouring water), we improve the planning pipeline by injecting some prior design and practical techniques (<i>e.g.</i>, spherical linear interpolation).
          On the other hand, we also enrich the diversity of data by collecting 2k human annotations and performing further augmentations.
          After validity examination, we finally collect 10k expert demonstrations in total.
          There are 7 data splits in each task: <i>Train</i>/<i>Val</i>/<i>Test</i> for <i>i.i.d.</i> scheme, novel <i>Object</i>/<i>Scene</i>/<i>State</i> for unseen generalization, and an extra split (<i>State</i><sup>*</sup>) with arbitrary continuous goal state.
          To provide diverse human language instructions, we prepare a template pool for each task.
          Each template has several placeholders that can be lexicalized with various equivalent phrases.
          Here we show the overview of <tt>ARNOLD</tt> data and a few instruction examples.
        </p>
        <br/>
        <div style="display:flex;">
          <div style="width:45%; display:flex; justify-content:center; align-items:center;">
            <img src="assets/data.png" alt="Data overview">
          </div>

          <div style="width:3%; display:flex; justify-content:center; align-items:center;">
          </div>

          <div style="width:48%; display:flex; justify-content:center; align-items:center;">
            <img src="assets/language.png" alt="Language instructions">
          </div>
        </div>

        <div style="width: 70%; margin: 0 auto;">
          <video poster="" id="generalization" autoplay muted loop height="100%">
            <source src="assets/generalization.mp4" type="video/mp4">
          </video>
        </div>

        <h3 class="title is-4">Experiment</h3>
        <div class="has-text-centered" style="padding: 0em;">
          <p style="text-align: justify;">
            We evaluate two state-of-the-art language-conditioned robotic manipulation models: <a target="_blank" href="https://sites.google.com/ucsc.edu/vlmbench/home">6D-CLIPort</a> and <a target="_blank" href="https://peract.github.io/">PerAct</a>.
            To better investigate the performances, we add several model variants:
            (1) PerAct without language (PerAct w/o L);
            (2) PerAct with additional supervision on state value (PerAct<sup>†</sup>);
            (3) Multi-task PerAct (PerAct MT).
            In addition, we provide the evaluation results with first-phase ground truth, <i>i.e.</i>, ground truth grasping.
          </p>
          <br/>
        </div>
        <div style="padding: 0em; width:80%; margin: 0 auto;">
          <p style="text-align: justify; font-size: 0.8em;">
            Evaluation results of the models on various tasks and splits, measured by success rate and shown in percentages.
            The gray figures indicate performances with the first-phase ground truth.
            For each model, the first row shows the performance on the <i>Test</i> set, and the following three rows show those on the novel splits of <i>Object</i>, <i>Scene</i>, and <i>State</i>.
            The last row indicates the performances on the <i>Any State</i> split.
            Tasks are abbreviated for more space. Average performances on eight tasks are appended to each row.
            w/o L: without language instruction. <sup>†</sup>: model variants with state modeling. MT: multi-task models.
          </p>
          <br/>
          <img src="assets/results.png" style="margin-top: -1rem;"/>
        </div>
        <br/>

        <h3 class="title is-4">Sim2Real</h3>
        <div class="has-text-centered">
          <p style="text-align: justify;">
            we set up a real-world environment for testing the Sim2Real transfer capabilities.
            Specifically, we select models from single-task PerAct and use a Franka robot arm to manipulate previously unseen real-world objects with a single RGB-D camera from the left view.
            We experiment with 2 different drawers and 5 different objects.
            The results show that models trained in <tt>ARNOLD</tt> show preliminary Sim2Real transfer capabilities; <i>i.e.</i>, reasonable predictions for both picking up objects and manipulating drawers.
            However, the Sim2Real gap remains, <i>e.g.</i>, strict friction.
            We hope the flexible design and realistic simulation in <tt>ARNOLD</tt> can gradually close this gap with more diverse and fine-grained object assets.
          </p>
          <br/>
        </div>

        <div style="display:flex;">
          <div style="width:2%; display:flex; justify-content:center; align-items:center;">
          </div>

          <div style="width:60%; display:flex; justify-content:center; align-items:center;">
            <img src="assets/real.png" alt="Sim2Real">
          </div>

          <div style="width:5%; display:flex; justify-content:center; align-items:center;">
          </div>

          <div style="width:30%; display:flex; justify-content:center; align-items:center;">
            <video poster="" id="real_video" autoplay muted loop height="100%">
              <source src="assets/real_video.mp4" type="video/mp4">
            </video>
          </div>
        </div>
        <br/>

        <h3 class="title is-4" style="margin-top: 2rem;">Conclusion</h3>
        <div class="has-text-centered">
          <p style="text-align: justify;">
            We present <tt>ARNOLD</tt>, a benchmark for language-grounded task learning in realistic 3D interactive environments with diverse scenes, objects, and continuous object states.
            We devise a systematic benchmark comprising eight challenging language-grounded robot tasks and evaluation splits for robot skill generalization in novel scene, object, and goal-state scenarios.
            We conduct extensive experiments and analyses to pinpoint the limitations of the current models and identify promising research directions for grounded task learning.
          </p>
        </div>
      </div>
    </div>
    </div>
  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-widescreen content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{gong2023arnold,
  title={ARNOLD: A Benchmark for Language-Grounded Task Learning With Continuous States in Realistic 3D Scenes},
  author={Gong, Ran and Huang, Jiangyong and Zhao, Yizhou and Geng, Haoran and Gao, Xiaofeng and Wu, Qingyang and Ai, Wensi and Zhou, Ziheng and Terzopoulos, Demetri and Zhu, Song-Chun and Jia, Baoxiong and Huang, Siyuan},
  journal={arXiv preprint arXiv:2304.04321},
  year={2023}
}</code></pre>
  </div>
</section>


</body>
</html>
